model:
  backbone: "meta-llama/Llama-2-7b-hf"
  freeze_layers: 6
  hidden_dim: 4096          # llama‑2‑7b
  num_centroids: 256
  num_operator_flows: 32
  flow_depth: 2
  tau_role: 0.07
  gamma_mem: 0.95

loss:
  lambda_sym: 0.5
  lambda_flow: 1.0
  lambda_self: 0.5

train:
  dataset: "data/corpus.txt"
  max_seq: 2048
  batch_size: 2
  lr: 2.0e-5
  warmup: 500
  steps: 100000
  ckpt_dir: "checkpoints"
  log_every: 20
  save_every: 2000
  device: "cuda" 