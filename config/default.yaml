model:
  backbone: "gpt2"
  freeze_layers: 6
  hidden_dim: 768
  num_centroids: 256
  num_operator_flows: 32
  flow_depth: 2
  tau_role: 0.07
  gamma_mem: 0.95

loss:
  lambda_sym: 0.5
  lambda_flow: 1.0
  lambda_self: 0.5

train:
  # Dataset settings
  datasets:  # List of datasets with their configurations
    - name: "papers"
      subset: "pubmed"  # Use pubmed subset of scientific papers
  # Example of using multiple datasets:
  # datasets:
  #   - name: "wikitext"
  #     subset: "wikitext-103-v1"
  #   - name: "code"
  #     subset: "python"
  #   - name: "papers"
  #     subset: "pubmed"
  max_seq: 128
  batch_size: 8
  
  # Training parameters
  lr: 2.0e-5
  warmup: 1000
  steps: 50000  # Total training steps
  
  # Logging and checkpoints
  ckpt_dir: "checkpoints"
  log_every: 10
  save_every: 1000
  
  # Hardware
  device: "mps" 